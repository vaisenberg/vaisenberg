{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Calculating Required Sample Size\n"
      ],
      "metadata": {
        "id": "JTlZAl2KfeWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> an A/B test to evaluate the impact of a new email subject line on the open rate. Based on past data, you expect a small effect size of 0.3 (an increase from 20% to 23% in the open rate). You aim for an 80% chance (power = 0.8) of detecting this effect if it exists, with a 5% significance level (α = 0.05).\n",
        "Calculate the required sample size per group using Python’s statsmodels library.\n",
        "What sample size is needed for each group to ensure your test is properly powered?\n",
        "\n"
      ],
      "metadata": {
        "id": "rI0cb2o8foTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "main additional assumptions for the A/B test:\n",
        "\n",
        "\n",
        "Equal Group Sizes:\n",
        "ratio=1: both groups in the A/B test (Group A and Group B) will have the same number of participants in order to provide the most statistical power for a given total sample size.\n",
        "\n",
        "Varience in Group Size - Minimized\n",
        "\n"
      ],
      "metadata": {
        "id": "pUTw8U6fgeZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Creating a TTestIndPower object\n",
        "effect_size = 0.3\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "ratio = 1\n",
        "\n",
        "analysis = TTestIndPower()\n",
        "\n",
        "# Calculating required sample size per group\n",
        "sample_size_per_group = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=ratio)\n",
        "\n",
        "print(f\"Required sample size per group: {sample_size_per_group:.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJfxzAFJf6LF",
        "outputId": "6a2fc53d-a417-449b-e9dc-32f95c71d09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Exercise 2: Understanding the Relationship Between Effect Size and Sample Size"
      ],
      "metadata": {
        "id": "0br7MZuOiPeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Using the same A/B test setup as in Exercise 1:\n",
        "Calculate the required sample size for the following effect sizes: 0.2, 0.4, and 0.5, keeping the significance level and power the same.\n",
        "How does the sample size change as the effect size increases? Explain why this happens.\n",
        "\n"
      ],
      "metadata": {
        "id": "tkziIqTLiVQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Creating a TTestIndPower object\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "ratio = 1\n",
        "\n",
        "# Effect sizes:\n",
        "effect_sizes = [0.2, 0.4, 0.5]\n",
        "analysis = TTestIndPower()\n",
        "\n",
        "# Calculating required sample sizes for each effect size\n",
        "sample_sizes = {effect_size: analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=ratio)\n",
        "                for effect_size in effect_sizes}\n",
        "for effect_size, sample_size in sample_sizes.items():\n",
        "    print(f\"Effect Size {effect_size}: Required Sample Size Per Group = {sample_size:.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DFfUBO6jloQ",
        "outputId": "269e99f0-7ea3-4033-b41d-9c494efcbb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effect Size 0.2: Required Sample Size Per Group = 393\n",
            "Effect Size 0.4: Required Sample Size Per Group = 99\n",
            "Effect Size 0.5: Required Sample Size Per Group = 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the effect size increases, the required sample size decreases. This happens because:\n",
        "1.   Larger Effects Are Easier to Detect:\n",
        "2.  A larger effect size means a bigger difference between the groups, making  \n",
        "it easier to detect the difference with fewer participants.\n",
        "3.  A larger effect size means a bigger difference between the groups, making it easier to detect the difference with fewer participants.\n",
        "\n",
        "Signal-to-Noise Ratio:\n",
        "A smaller effect size has a lower signal-to-noise ratio, requiring more data to distinguish the effect from random variation.\n",
        "\n",
        "**This demonstrates the inverse relationship between effect size and sample size in statistical testing.**"
      ],
      "metadata": {
        "id": "3NYOaPY7kn6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3: Exploring the Impact of Statistical Power"
      ],
      "metadata": {
        "id": "yuultBWMlGHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Imagine you are conducting an A/B test where you expect a small effect size of 0.2. You initially plan for a power of 0.8 but wonder how increasing or decreasing the desired power level impacts the required sample size.\n",
        "Calculate the required sample size for power levels of 0.7, 0.8, and 0.9, keeping the effect size at 0.2 and significance level at 0.05.\n",
        "Question: How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IejAtGpLlRUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Creating a TTestIndPower object\n",
        "effect_size = 0.2\n",
        "alpha = 0.05\n",
        "ratio = 1\n",
        "\n",
        "# different power levels to explore\n",
        "power_levels = [0.7, 0.8, 0.9]\n",
        "analysis = TTestIndPower()\n",
        "\n",
        "# Calculating required sample sizes for each power level\n",
        "sample_sizes_power = {power_level: analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power_level, ratio=ratio)\n",
        "                      for power_level in power_levels}\n",
        "\n",
        "for power_level, sample_size in sample_sizes_power.items():\n",
        "    print(f\"Power Level {power_level}: Required Sample Size Per Group = {sample_size:.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBUbGNbllb2N",
        "outputId": "c99243de-2d85-469f-ad6b-f9091a3aa8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power Level 0.7: Required Sample Size Per Group = 310\n",
            "Power Level 0.8: Required Sample Size Per Group = 393\n",
            "Power Level 0.9: Required Sample Size Per Group = 526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When designing A/B tests, setting an appropriate power level ensures the test has a good chance of detecting meaningful effects while avoiding unnecessary data collection.\n",
        "\n",
        "As the desired power increases, the required sample size also increases. This\n",
        "happens because:\n",
        "1.   Higher Power Means Greater Sensitivity: A higher statistical power reduces the probability of a Type II error (failing to detect an effect that actually exists). Achieving this sensitivity requires more data.\n",
        "2.   Balancing Resources: Understanding the relationship between power and sample size helps in balancing the need for reliable results against the cost and feasibility of data collection\n"
      ],
      "metadata": {
        "id": "aRmp4fAtmL8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Implementing Sequential Testing"
      ],
      "metadata": {
        "id": "K0XKSNJ6mvj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> You are running an A/B test on two versions of a product page to increase the purchase rate. You plan to monitor the results weekly and stop the test early if one version shows a significant improvement.\n",
        "Define your stopping criteria.\n",
        "Decide how you would implement sequential testing in this scenario.\n",
        "At the end of week three, Version B has a p-value of 0.02. What would you do next?\n",
        "\n"
      ],
      "metadata": {
        "id": "PL7f622Km_5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Stopping Criteria**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Stopping criteria for sequential testing involve rules to decide when to stop the test early:\n",
        "\n",
        "1. P-value Threshold: If the p-value drops below a predefined threshold (e.g., 0.05), the test can stop early with the conclusion that one version outperforms the other.\n",
        "2. Confidence Boundaries: Use adjusted boundaries (e.g., Bonferroni correction or alpha-spending) to control the overall Type I error rate when checking results multiple times.\n",
        "3. Maximum Duration: If no significant difference is found by a predefined endpoint, declare the test inconclusive."
      ],
      "metadata": {
        "id": "6z-YjOKdnp9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Sequential Testing in Code**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Jt3u0plVoOjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sequential_testing(p_values, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Simulate sequential testing by checking p-values at each stage.\n",
        "\n",
        "    Parameters:\n",
        "        p_values (list of float): List of p-values from each week's data.\n",
        "        alpha (float): Significance level threshold.\n",
        "\n",
        "    Returns:\n",
        "        str: Decision to stop or continue the test.\n",
        "    \"\"\"\n",
        "    for week, p_value in enumerate(p_values, start=1):\n",
        "        adjusted_alpha = alpha / week  # Bonferroni correction\n",
        "        if p_value < adjusted_alpha:\n",
        "            return f\"Stop the test: Significant result in Week {week} (p={p_value:.3f}, adjusted alpha={adjusted_alpha:.3f})\"\n",
        "    return \"Continue the test: No significant results yet.\"\n",
        "\n",
        "\n",
        "weekly_p_values = [0.10, 0.08, 0.01]\n",
        "decision = sequential_testing(weekly_p_values)\n",
        "\n",
        "print(decision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bauKOsVCn4cM",
        "outputId": "8aaead74-fbf4-4a41-cdad-0d591d040938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop the test: Significant result in Week 3 (p=0.010, adjusted alpha=0.017)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential testing increases the chance of a Type I error (false positive) if you use the standard\n",
        "α=0.05 threshold multiple times. Adjusting the alpha controls for this inflation."
      ],
      "metadata": {
        "id": "DlHgqLmVqWmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Exercise 5: Applying Bayesian A/B Testing"
      ],
      "metadata": {
        "id": "Gq2zvOdcql1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> You’re testing a new feature in your app, and you want to use a Bayesian approach. Initially, you believe the new feature has a 50% chance of improving user engagement. After collecting data, your analysis suggests a 65% probability that the new feature is better.\n",
        "Describe how you would set up your prior belief.\n",
        "After collecting data, how does the updated belief (posterior distribution) influence your decision?\n",
        "What would you do if the posterior probability was only 55%?\n",
        "\n"
      ],
      "metadata": {
        "id": "5-MlsmQCquER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.**Setting Up the Prior Belief:**\n",
        "The prior belief is symmetric: the feature is equally likely to be better or worse. This is represented as a Beta(1, 1) distribution (a uniform prior).\n",
        "\n",
        "2.**Decision Based on Posterior Probability:**\n",
        "65% Probability: If the posterior probability that the new feature is better is 65%, this indicates moderate confidence. A threshold for decision-making (e.g., 95%) should guide whether to implement the feature.\n",
        "55% Probability: If the posterior probability is only 55%, this indicates minimal confidence in the feature's superiority, suggesting insufficient evidence to roll out the feature.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pktzRvG5rX4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import beta\n",
        "\n",
        "def bayesian_ab_test(alpha_prior, beta_prior, successes, trials, threshold=0.95):\n",
        "    \"\"\"\n",
        "    Perform a Bayesian A/B test.\n",
        "\n",
        "    Parameters:\n",
        "        alpha_prior (int): Alpha parameter for the prior distribution (successes).\n",
        "        beta_prior (int): Beta parameter for the prior distribution (failures).\n",
        "        successes (int): Number of successes for the new feature.\n",
        "        trials (int): Total trials for the new feature.\n",
        "        threshold (float): Probability threshold for decision-making.\n",
        "\n",
        "    Returns:\n",
        "        str: Decision based on posterior probability.\n",
        "    \"\"\"\n",
        "    # Updating posterior parameters\n",
        "    alpha_post = alpha_prior + successes\n",
        "    beta_post = beta_prior + trials - successes\n",
        "\n",
        "    # Calculating posterior probability that the new feature is better\n",
        "    posterior_mean = beta(alpha_post, beta_post).mean()\n",
        "    prob_feature_better = 1 - beta(alpha_post, beta_post).cdf(0.5)  # Probability > 0.5\n",
        "\n",
        "    if prob_feature_better > threshold:\n",
        "        return f\"Roll out the feature: Posterior Probability = {prob_feature_better:.2f}\"\n",
        "    else:\n",
        "        return f\"Hold off: Posterior Probability = {prob_feature_better:.2f}\"\n",
        "\n",
        "# Example: Prior belief and observed data\n",
        "alpha_prior = 1\n",
        "beta_prior = 1\n",
        "successes = 65  # Engagements\n",
        "trials = 100    # Total users\n",
        "\n",
        "# Perform Bayesian A/B test\n",
        "decision = bayesian_ab_test(alpha_prior, beta_prior, successes, trials, threshold=0.95)\n",
        "print(decision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHhK0dN4q3B2",
        "outputId": "5f9d6068-a9ad-424e-e6f5-cfa2463de68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roll out the feature: Posterior Probability = 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It indicates an extremely high level of confidence (essentially 100%) that the new feature is better than the existing one based on the data."
      ],
      "metadata": {
        "id": "0St0JukLt_tZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6: Implementing Adaptive Experimentation"
      ],
      "metadata": {
        "id": "J4nU0Vjtuecy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’re running a test with three different website layouts to increase user engagement. Initially, each layout gets 33% of the traffic. After the first week, Layout C shows higher engagement.\n",
        "\n",
        "Explain how you would adjust the traffic allocation after the first week.\n",
        "Describe how you would continue to adapt the experiment in the following weeks.\n",
        "What challenges might you face with adaptive experimentation, and how would you address them?\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FVqMN8oHupIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusting Traffic Allocation After the First Week\n",
        "After the first week, when Layout C shows higher engagement,it is better implement adaptive experimentation by dynamically allocating more traffic to the better-performing layout(s). One common method is Thompson Sampling, a Bayesian approach that balances exploration (testing underperforming layouts) and exploitation (favoring the best-performing layout).\n",
        "\n",
        "To take Steps to Adjust Traffic:\n",
        "\n",
        "Update Performance Beliefs:\n",
        "Calculate the posterior probability for each layout’s performance based on engagement data (e.g., using a Beta distribution).\n",
        "\n",
        "Reallocate Traffic:\n",
        "Assign traffic proportionally to the probability of success for each layout. For example, if Layout C has a 70% chance of being the best, it should receive 70% of the traffic.\n",
        "\n",
        "Continue Monitoring:\n",
        "Reassess traffic allocation weekly or after collecting sufficient data to update posterior probabilities.\n",
        "\n",
        "---\n",
        "Adapting the Experiment in Following Weeks\n",
        "As the experiment progresses:\n",
        "\n",
        "Frequent Updates:\n",
        "Weekly or bi-weekly updates to traffic allocation based on the latest performance data.\n",
        "Explore Alternatives:\n",
        "Ensure underperforming layouts receive minimal, but non-zero, traffic to allow for potential performance changes over time.\n",
        "Dynamic Stopping:\n",
        "Stop testing layouts with consistently poor performance to focus traffic on top candidates.\n",
        "Combine Metrics:\n",
        "Use engagement and secondary metrics (e.g., conversions) to refine allocation if multiple outcomes are important.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dks7OMV3xDJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import beta\n",
        "\n",
        "def adaptive_allocation(successes, trials, total_traffic=100):\n",
        "    \"\"\"\n",
        "    Allocate traffic adaptively using Thompson Sampling.\n",
        "\n",
        "    Parameters:\n",
        "        successes (list): List of successes (engagements) for each layout.\n",
        "        trials (list): List of trials (users tested) for each layout.\n",
        "        total_traffic (int): Total traffic to allocate.\n",
        "\n",
        "    Returns:\n",
        "        list: Traffic allocation percentages for each layout.\n",
        "    \"\"\"\n",
        "    # Sampling from Beta distributions for each layout\n",
        "    sampled_means = [np.random.beta(s + 1, t - s + 1) for s, t in zip(successes, trials)]\n",
        "\n",
        "    # Calculating allocation proportions\n",
        "    probabilities = np.array(sampled_means) / np.sum(sampled_means)\n",
        "\n",
        "    # Distributing total traffic based on probabilities\n",
        "    allocation = (probabilities * total_traffic).astype(int)\n",
        "    return allocation\n",
        "\n",
        "# Example: Data after Week 1\n",
        "successes = [30, 25, 50]  # Layout A, B, C\n",
        "trials = [100, 100, 100]\n",
        "\n",
        "# Calculating new traffic allocation\n",
        "allocation = adaptive_allocation(successes, trials, total_traffic=100)\n",
        "print(f\"Traffic Allocation: {allocation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4posQBct_Ci",
        "outputId": "cd258ad4-74a1-4031-e193-139c3e0e0eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traffic Allocation: [23 24 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenges of Adaptive Experimentation**\n",
        "1. Exploration vs. Exploitation Trade-Off:\n",
        "Challenge: Over-allocating traffic to the leading layout early may prevent learning about underperforming layouts that could improve later.\n",
        "Solution: Use algorithms like Thompson Sampling or UCB (Upper Confidence Bound) that ensure a balance between exploration and exploitation.\n",
        "2. Statistical Validity:\n",
        "Challenge: Frequent changes in traffic allocation can make traditional statistical methods (e.g., p-values) invalid.\n",
        "Solution: Use Bayesian methods or simulation-based approaches to calculate confidence in layout performance.\n",
        "3. Noise and Variability:\n",
        "Challenge: Early performance differences might be due to random chance rather than true superiority.\n",
        "Solution: Set a minimum data threshold before adapting traffic allocation (e.g., 500 users per layout).\n",
        "4. Implementation Complexity:\n",
        "Challenge: Adaptive experiments require real-time data processing and automated traffic allocation.\n",
        "Solution: Use tools like multi-armed bandit frameworks or experimentation platforms (e.g., Google Optimize, Optimizely).\n"
      ],
      "metadata": {
        "id": "FS7biO74x6ml"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yz2fBvtLtfed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}